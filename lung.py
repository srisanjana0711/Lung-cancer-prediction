# -*- coding: utf-8 -*-
"""lung.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WMvxC_inJ1FfTBhBUCjxhNOavMxIeV6L
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import cv2
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Load CSV file containing image paths and labels
csv_file = '/content/drive/MyDrive/Lung/Dset/dset_labels.csv'  # Update with your CSV file path
data = pd.read_csv(csv_file)

# Define the base path where the images are stored in Google Drive
base_path = '/content/drive/My Drive/Lung/Dset/'

# Function to update the paths
def update_image_paths(row):
    folder = row['Image_Path'].split('/')[0]  # Get folder name like 'Bengin cases', 'Malignant cases'
    image = row['Image_Path'].split('/')[1]  # Get image name like 'Bengin case (57).jpg'
    return base_path + folder + '/' + image

# Apply the function to update the 'Image_Path' column
data['Image_Path'] = data.apply(update_image_paths, axis=1)

# Save the updated CSV file
updated_csv_file = '/content/drive/My Drive/Lung/updated_dset_label.csv'
data.to_csv(updated_csv_file, index=False)

print("CSV paths updated successfully!")

# Set image size and batch size
IMG_SIZE = 224
BATCH_SIZE = 32  # Number of images to process at once

# Function for noise reduction using Gaussian Blur
def reduce_noise(image):
    return cv2.GaussianBlur(image, (5, 5), 0)

# Function for contrast enhancement using CLAHE
def enhance_contrast(image):
    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    cl = clahe.apply(l)
    limg = cv2.merge((cl, a, b))
    return cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)

# Function to preprocess images (resize, noise reduction, contrast enhancement, normalization)
def preprocess_image(image_path):
    img = cv2.imread(image_path)
    if img is None:  # Check if the image was loaded correctly
        print(f"Error: Could not load image from path: {image_path}")
        return None
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))  # Resize image
    img = reduce_noise(img)  # Apply noise reduction
    img = enhance_contrast(img)  # Apply contrast enhancement
    img = img / 255.0  # Normalize pixel values (0-255 to 0-1)
    return img

# Generator to load and process images in batches
def data_generator(data, batch_size, augment=False):
    while True:
        for start in range(0, len(data), batch_size):
            batch_data = data[start:start + batch_size]
            images = []
            labels = []
            for index, row in batch_data.iterrows():
                image_path = row['Image_Path']
                label = row['Label']
                image = preprocess_image(image_path)  # Preprocess the image

                if image is None:
                    continue  # Skip if image could not be loaded

                if augment:
                    # Apply data augmentation
                    image = image_augmentor.random_transform(image)

                images.append(image)
                labels.append(label)

            # Debugging: Print shapes
            if len(images) > 0:
                print(f"Batch images shape: {np.array(images).shape}")
                print(f"Batch labels shape: {np.array(labels).shape}")

            yield np.array(images), np.array(labels)

# Data augmentation: random rotations, flips, zoom
image_augmentor = ImageDataGenerator(
    rotation_range=30,   # Rotate images randomly up to 30 degrees
    horizontal_flip=True,  # Flip images horizontally
    zoom_range=0.2  # Random zoom in the images
)

# Define CNN model
def build_cnn_model(input_shape):
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        MaxPooling2D((2, 2)),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Conv2D(128, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Flatten(),
        Dense(512, activation='relu'),
        Dropout(0.5),
        Dense(1, activation='sigmoid')  # For binary classification
    ])
    model.compile(optimizer=Adam(),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model

input_shape = (IMG_SIZE, IMG_SIZE, 3)
cnn_model = build_cnn_model(input_shape)
cnn_model.summary()

# Split data into training and testing sets
from sklearn.model_selection import train_test_split

train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['Label'])

# Define data generators for training and testing
train_gen = data_generator(train_data, BATCH_SIZE, augment=True)
test_gen = data_generator(test_data, BATCH_SIZE, augment=False)

# Train the CNN model
history = cnn_model.fit(
    train_gen,
    steps_per_epoch=len(train_data) // BATCH_SIZE,
    epochs=5,
    validation_data=test_gen,
    validation_steps=len(test_data) // BATCH_SIZE
)

test_loss, test_accuracy = cnn_model.evaluate(test_gen, steps=len(test_data) // BATCH_SIZE)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

from sklearn.metrics import confusion_matrix, classification_report

# Predict on test set
y_true = []
y_pred = []
for batch_images, batch_labels in test_gen:
    preds = cnn_model.predict(batch_images)
    y_true.extend(batch_labels)
    y_pred.extend((preds > 0.5).astype(int))  # Convert probabilities to binary labels
    if len(y_true) >= len(test_data):
        break

y_true = np.array(y_true)
y_pred = np.array(y_pred)

cm = confusion_matrix(y_true, y_pred)
cr = classification_report(y_true, y_pred)
print("Confusion Matrix:\n", cm)
print("Classification Report:\n", cr)

cnn_model.save('/content/drive/MyDrive/Lung/cnn_model.h5')

from tensorflow.keras.models import load_model
cnn_model = load_model('/content/drive/MyDrive/Lung/cnn_model.h5')

cnn_model.summary()

import tensorflow as tf
import numpy as np

# Assuming 'cnn_model' is your CNN model
# Ensure the model is built and compiled

dummy_input = np.zeros((1, 224, 224, 3))
_ = cnn_model(dummy_input)  # Call the model on the dummy input

# Access the input layer of the trained model
input_layer = cnn_model.layers[0].input

# Define a model to extract features from the penultimate layer
feature_extractor_cnn_model = tf.keras.Model(inputs=input_layer, outputs=cnn_model.layers[-2].output)

# Print the summary of the feature extractor to verify
feature_extractor_cnn_model.summary()

# Function to extract features from images using the feature extractor model
def extract_features_cnn_model(image_paths):
    features = []
    for image_path in image_paths:
        img = preprocess_image(image_path)  # Preprocess the image as in your current code
        if img is not None:
            img = np.expand_dims(img, axis=0)  # Add batch dimension
            feature = feature_extractor_cnn_model.predict(img)  # Extract features
            features.append(feature.flatten())  # Flatten the features and append to the list
    return np.array(features)

# Example usage: Extract features for a batch of images
example_image_paths = test_data['Image_Path'].tolist()  # Get the paths from your test data
features_cnn_model = extract_features_cnn_model(example_image_paths)

print(f"Extracted features shape from penultimate layer: {features_cnn_model.shape}")

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Assuming `features_cnn_model` contains the extracted features from CNN for the test data
# And `test_labels` contains the corresponding labels for the test data

# Split the features and labels into training and testing sets
# Assuming you already have training data features extracted using the same method
train_features = extract_features_cnn_model(train_data['Image_Path'].tolist())
test_features = features_cnn_model  # Features from the test data
train_labels = train_data['Label'].values
test_labels = test_data['Label'].values

# Initialize and train the Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(train_features, train_labels)

# Make predictions on the test data
test_predictions = rf_model.predict(test_features)

# Evaluate the model performance
accuracy = accuracy_score(test_labels, test_predictions)
print(f"Random Forest Accuracy: {accuracy * 100:.2f}%")

# Detailed classification report
print("Classification Report:")
print(classification_report(test_labels, test_predictions))

# Confusion matrix to see how well the classifier performs for each class
print("Confusion Matrix:")
print(confusion_matrix(test_labels, test_predictions))

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Assuming you already have 'features_cnn_model' (CNN features) and 'labels'

# Define the labels - it seems like you want to use the labels from the test_data
labels = test_data['Label'].values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features_cnn_model, labels, test_size=0.2, random_state=42)

# Define the base models
base_learners = [
    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('svm', SVC(probability=True))
]

# Meta-classifier
meta_classifier = LogisticRegression()

# Stacking ensemble
stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=meta_classifier)

# Train the ensemble model
stacking_clf.fit(X_train, y_train)

# Make predictions
y_pred = stacking_clf.predict(X_test)

# Evaluate the ensemble model
accuracy = accuracy_score(y_test, y_pred)
print(f"Stacking Ensemble Accuracy: {accuracy * 100:.2f}%")
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Assuming you have predictions from different models
# Example predictions (probabilities) from base models
# These should be the predicted probabilities (not labels) for the test set

# Model predictions (probabilities) on the test set
#cnn_predictions = cnn_model.predict(X_test)  # CNN model predictions - This line is causing the error
rf_predictions = rf_model.predict_proba(X_test)  # Random Forest model predictions

# Assign weights based on model performance
# For example, if CNN accuracy is 95% and RF accuracy is 90%
#cnn_weight = 0.6  # Weight for CNN model - Commenting out since we are not using CNN predictions
rf_weight = 1.0  # Weight for Random Forest model - Setting to 1.0 since this is the only model

# Combine predictions using weighted average
# Adjusting shape to ensure proper weighted average calculation
#combined_predictions = (cnn_weight * cnn_predictions + rf_weight * rf_predictions) - This line is causing the error
combined_predictions = rf_weight * rf_predictions # Using only RF predictions

# Convert probabilities to class labels (select the class with highest probability)
final_predictions = np.argmax(combined_predictions, axis=1)

# Evaluate the performance
accuracy = accuracy_score(y_test, final_predictions)
report = classification_report(y_test, final_predictions)
conf_matrix = confusion_matrix(y_test, final_predictions)

print(f"Weighted Average Ensemble Accuracy: {accuracy * 100:.2f}%")
print("Classification Report:")
print(report)
print("Confusion Matrix:")
print(conf_matrix)

from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.base import BaseEstimator, ClassifierMixin

# Assuming you have trained models: cnn_model, rf_model, etc.
# Convert Keras model to a scikit-learn compatible estimator

class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, model, **kwargs):
        self.model = model
        self.kwargs = kwargs

    def fit(self, X, y):
        # Ensure the input data is in the correct format for the CNN model
        #X = X.reshape((-1, 224, 224, 3))  # Reshape to (num_samples, 224, 224, 3) - Removing this line as it is causing the error
        self.model.fit(X, y, **self.kwargs)
        return self

    def predict(self, X):
        # Ensure the input data is in the correct format for the CNN model
        #X = X.reshape((-1, 224, 224, 3))  # Reshape to (num_samples, 224, 224, 3) - Removing this line as it is causing the error
        return (self.model.predict(X) > 0.5).astype(int).flatten()

# Wrap your models
#keras_model = KerasClassifierWrapper(cnn_model, epochs=10, batch_size=32) - Commenting out since this is causing the error
rf_model = rf_model  # Assuming this is already trained

# Create a voting classifier
voting_clf = VotingClassifier(estimators=[
    #('cnn', keras_model), - Commenting out since this is causing the error
    ('rf', rf_model)
], voting='hard')  # Use 'hard' for hard voting

# Fit the voting classifier on training data
# Ensure that X_train is the original image data, not the extracted features
voting_clf.fit(X_train, y_train)

# Predict on test data
# Ensure that X_test is the original image data, not the extracted features
final_predictions = voting_clf.predict(X_test)

# Evaluate the performance
accuracy = accuracy_score(y_test, final_predictions)
report = classification_report(y_test, final_predictions)
conf_matrix = confusion_matrix(y_test, final_predictions)

print(f"Voting Ensemble Accuracy: {accuracy * 100:.2f}%")
print("Classification Report:")
print(report)
print("Confusion Matrix:")
print(conf_matrix)

import joblib

# Save the Random Forest model
joblib.dump(rf_model, 'random_forest_model.pkl')

print("Random Forest model saved successfully!")

import joblib

# Load the Random Forest model
loaded_rf_model = joblib.load('random_forest_model.pkl')

print("Random Forest model loaded successfully!")

import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

# Load the Random Forest model
loaded_rf_model = joblib.load('random_forest_model.pkl')
print("Random Forest model loaded successfully!")

# Assuming you have test data in X_test and y_test
# Predict probabilities using the loaded model
y_pred_prob = loaded_rf_model.predict_proba(X_test)  # Get predicted probabilities

# Convert probabilities to class labels
y_pred = np.argmax(y_pred_prob, axis=1)

# Compute metrics
report = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'], output_dict=True)
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot classification report
def plot_classification_report(report):
    plt.figure(figsize=(5, 3))
    report_df = pd.DataFrame(report).iloc[:-1, :].T  # Remove 'accuracy' row for better visualization
    sns.heatmap(report_df, annot=True, cmap='Blues', fmt='.2f')
    plt.title('Classification Report')
    plt.show()

# Plot confusion matrix
def plot_confusion_matrix(conf_matrix, class_names):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

# Plot the metrics
plot_classification_report(report)
plot_confusion_matrix(conf_matrix, class_names=['Class 0', 'Class 1'])

# Print performance metrics
print("Classification Report:\n", classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))

import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd

# Mount Google Drive (if using Colab)
from google.colab import drive
drive.mount('/content/drive')

# Path to save images in Google Drive
save_path = '/content/drive/MyDrive/Lung/'  # Adjust the path as needed

# Load the Random Forest model
loaded_rf_model = joblib.load('random_forest_model.pkl')
print("Random Forest model loaded successfully!")

# Assuming you have test data in X_test and y_test
# Predict probabilities using the loaded model
y_pred_prob = loaded_rf_model.predict_proba(X_test)  # Get predicted probabilities

# Convert probabilities to class labels
y_pred = np.argmax(y_pred_prob, axis=1)

# Compute metrics
report = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'], output_dict=True)
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot classification report
def plot_classification_report(report, save=False):
    plt.figure(figsize=(5, 3))
    report_df = pd.DataFrame(report).iloc[:-1, :].T  # Remove 'accuracy' row for better visualization
    sns.heatmap(report_df, annot=True, cmap='Blues', fmt='.2f')
    plt.title('Classification Report Of Proposed System')

    if save:
        plt.savefig(save_path + 'classification_report.png')  # Save the plot to Google Drive
    plt.show()

# Plot confusion matrix
def plot_confusion_matrix(conf_matrix, class_names, save=False):
    plt.figure(figsize=(4, 3))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')

    if save:
        plt.savefig(save_path + 'confusion_matrix.png')  # Save the plot to Google Drive
    plt.show()

# Plot the metrics and save images
plot_classification_report(report, save=True)  # Save classification report
plot_confusion_matrix(conf_matrix, class_names=['Class 0', 'Class 1'], save=True)  # Save confusion matrix

# Print performance metrics
print("Classification Report:\n", classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score

# Assuming you have your test data and model predictions
# y_test: true labels
# y_pred_prob: predicted probabilities from the model

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:, 1])  # Use probabilities for class 1

# Compute the AUC
roc_auc = roc_auc_score(y_test, y_pred_prob[:, 1])

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid(True)

# Show the plot
plt.show()

# Save the ROC curve plot to Google Drive (if using Colab and mounted drive)
plt.savefig('/content/drive/MyDrive/Lung/roc_curve.png')

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Example dataset creation (you can replace this with your actual dataset)
# Replace this with actual data loading
# Example: X (features) and y (binary labels)
# X should be your input features, and y should be the corresponding labels (0 or 1)

# For demonstration purposes, let's generate a random dataset
X = np.random.rand(100, 10)  # 100 samples, 10 features
y = np.random.randint(0, 2, size=100)  # Binary target (0 or 1)

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a RandomForest model
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# Predict probabilities (for ROC calculation)
y_pred_proba = rf_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class (1)

# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')

# Plot a diagonal dashed line to show a model with random guessing (AUC = 0.5)
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Guessing (AUC = 0.5)')

# Add labels and title
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')

# Save the plot to Google Drive (or any desired location)
plt.savefig('/content/drive/My Drive/Lung/roc_auc_curve.png')  # Change path as needed

# Show the plot
plt.show()

# Print AUC score for the model
print(f"AUC Score: {roc_auc:.2f}")

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import pandas as pd

# Load your dataset
# Assuming you already have X (features) and y (labels) ready
# X, y = ... (use your actual dataset here)

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=10, random_state=42)
rf_model.fit(X_train, y_train)

# Function to calculate Gini index for each tree
def calculate_gini(tree):
    tree_structure = tree.tree_
    gini_values = tree_structure.impurity  # Gini index for each node in the tree
    return gini_values

# Iterate through each tree in the Random Forest
for tree_idx, tree in enumerate(rf_model.estimators_):
    print(f"\nGini Index for Decision Tree {tree_idx + 1}:")
    gini_values = calculate_gini(tree)

    # Display the Gini values for all nodes in the tree
    for node_idx, gini_value in enumerate(gini_values):
        print(f"Node {node_idx}: Gini = {gini_value:.4f}")

# Evaluate the Random Forest model on the test data
rf_predictions = rf_model.predict(X_test)

import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import pandas as pd

# Load your dataset
# Assuming you already have X (features) and y (labels) ready
# X, y = ... (use your actual dataset here)

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=10, random_state=42)
rf_model.fit(X_train, y_train)

# Function to calculate Gini index for each tree
def calculate_gini(tree):
    tree_structure = tree.tree_
    gini_values = tree_structure.impurity  # Gini index for each node in the tree
    return gini_values

# Plot the Gini index for a specific tree in the Random Forest
def plot_gini_index(tree_idx):
    gini_values = calculate_gini(rf_model.estimators_[tree_idx])
    nodes = np.arange(len(gini_values))

    plt.figure(figsize=(5, 3))
    plt.plot(nodes, gini_values, marker='o', linestyle='-', color='b')
    plt.title(f'Gini Index for Decision Tree {tree_idx + 1}')
    plt.xlabel('Node Index')
    plt.ylabel('Gini Index')
    plt.grid(True)
    plt.show()

# Example: Plot Gini index for the first decision tree
plot_gini_index(tree_idx=0)

# Save the plot to Google Drive
plt.savefig('/content/drive/My Drive/Lung/gini_index_plot.png', dpi=300, bbox_inches='tight')